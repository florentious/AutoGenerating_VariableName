{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0Va1eAeAj7V"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6f_CsEoBSOw"
   },
   "outputs": [],
   "source": [
    "# don't install already mxnet\n",
    "pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ua0E3g2aMCJc"
   },
   "outputs": [],
   "source": [
    "pip install mxnet-mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsWMPO4wMiB-"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m20X2wg9MVBE"
   },
   "outputs": [],
   "source": [
    "pip install mxnet-cu101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_w0YJQ5MuHz"
   },
   "outputs": [],
   "source": [
    "pip install mxnet-cu101mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6vG5qKIBeYY"
   },
   "outputs": [],
   "source": [
    "pip install gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BqBPrumBMMH"
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "\n",
    "import mxnet as mx\n",
    "import gensim\n",
    "import tqdm\n",
    "import gluonnlp\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn,rnn\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "from mxnet import ndarray\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qaVNelmBMOW"
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "\n",
    "base_path = 'drive/My Drive/pythonWorkspace/DeepLearning/AutoGenerating_VariableName/'\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ4GOt2wBMTP"
   },
   "outputs": [],
   "source": [
    "# modeling class\n",
    "\n",
    "class auto_spacing(gluon.HybridBlock) :\n",
    "    def __init__(self, n_hidden, vocab_size, embed_dim, max_seq_length,**kwargs) :\n",
    "        super(auto_spacing, self).__init__(**kwargs)\n",
    "        \n",
    "        self.in_seq_len = max_seq_length\n",
    "        self.out_seq_len = max_seq_length\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        with self.name_scope() :\n",
    "            self.embedding = nn.Embedding(input_dim=self.vocab_size,output_dim=self.embed_dim)\n",
    "            \n",
    "            self.conv_unigram = nn.Conv2D(channels=128,in_channels=1, kernel_size=(1, self.embed_dim))\n",
    "            self.conv_bigram = nn.Conv2D(channels=256, in_channels=1, kernel_size=(2, self.embed_dim), padding=(1, 0))\n",
    "            self.conv_trigram = nn.Conv2D(channels=128, in_channels=1, kernel_size=(3, self.embed_dim), padding=(1, 0))\n",
    "            self.conv_forthgram = nn.Conv2D(channels=64, in_channels=1,  kernel_size=(4, self.embed_dim), padding=(2, 0))\n",
    "            self.conv_fifthgram = nn.Conv2D(channels=32,in_channels=1,  kernel_size=(5, self.embed_dim), padding=(2, 0)) \n",
    "\n",
    "            self.bi_gru = rnn.GRU(hidden_size=self.n_hidden,input_size=608, layout='NTC', bidirectional=True)\n",
    "            self.dense_sh = nn.Dense(100, in_units=608, activation='relu', flatten=False)\n",
    "            self.dense = nn.Dense(1,in_units=100, activation='sigmoid', flatten=False)\n",
    "    \n",
    "    def hybrid_forward(self, F, inputs) :\n",
    "        embed = self.embedding(inputs)\n",
    "        embed = F.expand_dims(embed, axis=1)\n",
    "\n",
    "        unigram = self.conv_unigram(embed)\n",
    "        bigram = self.conv_bigram(embed)\n",
    "        trigram = self.conv_trigram(embed)\n",
    "        forthgram = self.conv_forthgram(embed)\n",
    "        fifthgram = self.conv_fifthgram(embed)\n",
    "        \n",
    "        grams = F.concat(unigram, F.slice_axis(bigram, axis=2, begin=0,end=self.max_seq_length),\n",
    "                        trigram, F.slice_axis(forthgram, axis=2, begin=0,end=self.max_seq_length), \n",
    "                        F.slice_axis(fifthgram, axis=2, begin=0,end=self.max_seq_length), dim=1)\n",
    "        \n",
    "        grams = F.transpose(grams, (0,2,3,1))\n",
    "        grams = F.reshape(grams, (-1,self.max_seq_length,-3))\n",
    "        grmas = self.bi_gru(grams)\n",
    "        fc1 = self.dense_sh(grams)\n",
    "        return (self.dense(fc1))\n",
    "\n",
    "\n",
    "def model_init(n_hidden,vocab_size, embed_dim, max_seq_length, ctx, embed_weights) :\n",
    "    model = auto_spacing(n_hidden, vocab_size, embed_dim, max_seq_length)\n",
    "    \n",
    "    model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    model.embedding.weight.set_data(embed_weights)\n",
    "    model.hybridize(static_alloc=True)\n",
    "    \n",
    "    model.embedding.collect_params().setattr('grad_req', 'null')\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "    loss = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)\n",
    "    loss.hybridize(static_alloc=True)\n",
    "    return (model, loss, trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHjjIQ54BMVe"
   },
   "outputs": [],
   "source": [
    "# util\n",
    "\n",
    "def string2spacingChar(input_) :\n",
    "    # 빈칸을 '^'로 변환하기\n",
    "    chars = input_.strip().replace(' ','^')\n",
    "    \n",
    "    # SOS : «, EOS : »\n",
    "    tagged_chars = \"«\"+chars+\"»\"\n",
    "    \n",
    "    char_list = ' '.join(list(tagged_chars))\n",
    "    \n",
    "    return char_list\n",
    "\n",
    "def load_vocab(path) :\n",
    "    import json\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        word2idx = data\n",
    "        idx2word = dict([(v,k) for (k,v) in data.items()])\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "def pre_processing(input_) :\n",
    "    ch_list = []\n",
    "    \n",
    "    for cl in input_ :\n",
    "        ch_list.append(string2spacingChar(cl).replace(' ',''))\n",
    "        \n",
    "    return ch_list    \n",
    "\n",
    "def load_embedding(embeddings_file):\n",
    "    return (np.load(embeddings_file))\n",
    "\n",
    "def replaceEx(input_) :\n",
    "    # 이상한 글자제거\n",
    "    res = []\n",
    "    for str_ in input_ :\n",
    "        res.append(str_.replace('::','').replace('{','').replace('-',''))\n",
    "    return res    \n",
    "\n",
    "def mkdirs_(output_path) : \n",
    "        try :\n",
    "            if not os.path.exists(output_path) :\n",
    "                os.mkdir(output_path)\n",
    "        except :\n",
    "            print('Error : '+ output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NiHVCl-BMXv"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "class MySentenceGenerator(object) :\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    def __init__(self, fname) :\n",
    "        self.fname = fname\n",
    "    def __iter__(self) :\n",
    "        df_origin = pd.read_csv(base_path+'dataset/spacing.csv').origin\n",
    "        for idx in range(len(df_origin)) :\n",
    "            yield string2spacingChar(df_origin[idx].strip()).split(' ')\n",
    "\n",
    "def create_embedding(data_dir, model_file, embeddings_file, vocab_file, splitc=' ',**params) :\n",
    "    import json\n",
    "    from gensim.models import Word2Vec\n",
    "    \n",
    "    class SentenceGenerator(object) :\n",
    "        import os\n",
    "        def __init__(self, dirname) :\n",
    "            self.dirname = dirname\n",
    "        def __iter__(self) :\n",
    "            for fname in os.listdir(self.dirname) :\n",
    "                print(\"Processing~ '{}'\".format(fname))\n",
    "                with open(self.dirname+fname,'r',encoding='utf8') as f :\n",
    "                    for line in f.readlines() :\n",
    "                        yield string2spacingChar(line.strip()).split(splitc)\n",
    "                        \n",
    "    class SentenceGenerator_another(object) :\n",
    "        import os\n",
    "        import pandas as pd\n",
    "\n",
    "        def __init__(self, fname) :\n",
    "            self.fname = fname\n",
    "        def __iter__(self) :\n",
    "            df_origin = pd.read_csv(base_path+'dataset/spacing.csv').origin\n",
    "            for idx in range(len(df_origin)) :\n",
    "                yield string2spacingChar(df_origin[idx].strip()).split(' ')\n",
    "\n",
    "    sentences = SentenceGenerator(data_dir)\n",
    "    \n",
    "    model = Word2Vec(sentences,**params)\n",
    "    \n",
    "    model.save(model_file)\n",
    "    \n",
    "    weights = model.wv.syn0\n",
    "    default_vec = np.mean(weights, axis=0, keepdims = True)\n",
    "    padding_vec = np.zeros((1,weights.shape[1]))\n",
    "    \n",
    "    weights_default = np.concatenate([weights, default_vec, padding_vec], axis=0)\n",
    "    \n",
    "    np.save(open(embeddings_file, 'wb'), weights_default)\n",
    "    \n",
    "    vocab = dict([(k,v.index) for k,v in model.wv.vocab.items()])\n",
    "    vocab['__ETC__'] = weights_default.shape[0] -2\n",
    "    vocab['__PAD__'] = weights_default.shape[0] -1\n",
    "    \n",
    "    with open(vocab_file,'w') as f :\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "\n",
    "def pad_sequences(sequences,maxlen=None,dtype='int32',padding='pre',truncating='pre',value=0.):\n",
    "\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' %truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError(\n",
    "                'Shape of sample %s of sequence at position %s is different from expected shape %s'\n",
    "                % (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "def encoding_and_padding(word2idx_dic, sequences,**params) :\n",
    "    seq_idx = [[word2idx_dic.get(a,word2idx_dic['__ETC__']) for a in i] for i in sequences]\n",
    "    params['value'] = word2idx_dic['__PAD__']\n",
    "    return pad_sequences(seq_idx, **params)\n",
    "\n",
    "def get_transDtype(x,y,batch_size) :\n",
    "    from mxnet import gluon\n",
    "    tr_set = gluon.data.ArrayDataset(x,y.astype('float32'))\n",
    "    tr_data_iterator = gluon.data.DataLoader(tr_set,batch_size=batch_size)\n",
    "    return tr_data_iterator\n",
    "\n",
    "    \n",
    "def y_encoding(n_grams, maxlen=50):\n",
    "    init_mat = np.zeros(shape=(len(n_grams), maxlen), dtype=np.int8)\n",
    "    for i in range(len(n_grams)):\n",
    "        init_mat[i, np.cumsum([len(j) for j in n_grams[i]]) - 1] = 1\n",
    "    return init_mat\n",
    "\n",
    "def split_train_set(x_train, p=0.98):\n",
    "    \n",
    "    import numpy as np\n",
    "    train_idx = np.random.choice(range(x_train.shape[0]),int(x_train.shape[0] * p),replace=False)\n",
    "    set_tr_idx = set(train_idx)\n",
    "    test_index = [i for i in range(x_train.shape[0]) if i not in set_tr_idx]\n",
    "    return ((train_idx, np.array(test_index)))\n",
    "\n",
    "def input_data(input_path, train_ratio = 1,isMake_dif_set=False,max_seq_len=300,vocab_path=base_path+'ipynbCode/util/w2idx.dic',batch_size=100) :\n",
    "    import os\n",
    "    \n",
    "    # get_rawData\n",
    "    dir_list =  [txt for txt in os.listdir(input_path) if txt.endswith(\".txt\")]\n",
    "    \n",
    "    X_origin = []\n",
    "    for txt in dir_list :\n",
    "        with open(input_path+txt, 'r', encoding='utf8') as f:\n",
    "            X_origin.extend(f.readlines())\n",
    "            \n",
    "    # del '::','{', etc..\n",
    "    X_origin = replaceEx(X_origin)\n",
    "    # add <SoS>, <EoS>, replace(' ','^')\n",
    "    processed_seq_ = pre_processing(X_origin)\n",
    "    # del blank_list\n",
    "    processed_seq_ = list(filter(lambda x : x != \"«»\",processed_seq_))\n",
    "    # shuffle sequence list\n",
    "    samp_idx = np.random.choice(range(len(processed_seq_)), int(len(processed_seq_) * train_ratio), replace=False)\n",
    "    \n",
    "    processed_seq = [processed_seq_[i] for i in samp_idx]\n",
    "    sp_sents = [i.split('^') for i in processed_seq]\n",
    "    \n",
    "    # 8어절로 나눠서 테스트\n",
    "    if isMake_dif_set is True:\n",
    "        n_gram = [[k, v, z, a, c, d, e, f] for sent in sp_sents for k, v, z, a, c, d, e, f in zip(sent, sent[1:], sent[2:],\n",
    "                                                               sent[3:], sent[4:], sent[5:],sent[6:], sent[7:])]\n",
    "    else:\n",
    "        n_gram = sp_sents\n",
    "    \n",
    "    # make_target(space = 1, others = 0)\n",
    "    n_gram_y = y_encoding(n_gram, max_seq_len)\n",
    "    \n",
    "    w2idx, idx2w = load_vocab(vocab_path)\n",
    "    \n",
    "    # input seq - del blank\n",
    "    ngram_encoded_padded = encoding_and_padding(word2idx_dic=w2idx,\n",
    "                                                sequences=[''.join(gram) for gram in n_gram], maxlen=max_seq_len,\n",
    "                                                padding='post', truncating='post')\n",
    "    \n",
    "    if train_ratio < 1:\n",
    "        # make train_set\n",
    "        tr_idx, te_idx = split_train_set(ngram_encoded_padded, train_ratio)\n",
    "\n",
    "        y_train = n_gram_y[tr_idx, ]\n",
    "        x_train = ngram_encoded_padded[tr_idx, ]\n",
    "\n",
    "        y_test = n_gram_y[te_idx, ]\n",
    "        x_test = ngram_encoded_padded[te_idx, ]\n",
    "\n",
    "        # train generator\n",
    "        train_generator = get_transDtype(x_train, y_train, batch_size)\n",
    "        valid_generator = get_transDtype(x_test, y_test, 500)\n",
    "        return (train_generator, valid_generator)\n",
    "    else :\n",
    "        return get_transDtype(ngram_encoded_padded,n_gram_y,batch_size)\n",
    "\n",
    "def input_data_csv(input_path, train_ratio = 1,isMake_dif_set=False,max_seq_len=300,vocab_path=base_path+'ipynbCode/util/w2idx.dic',batch_size=100) :\n",
    "    import os\n",
    "    \n",
    "    # get_rawData\n",
    "    dir_list =  [txt for txt in os.listdir(input_path) if txt.endswith(\".txt\")]\n",
    "    \n",
    "    X_origin = []\n",
    "    for txt in dir_list :\n",
    "        with open(input_path+txt, 'r', encoding='utf8') as f:\n",
    "            X_origin.extend(f.readlines())\n",
    "            \n",
    "    # del '::','{', etc..\n",
    "    X_origin = replaceEx(X_origin)\n",
    "    # add <SoS>, <EoS>, replace(' ','^')\n",
    "    processed_seq_ = pre_processing(X_origin)\n",
    "    # del blank_list\n",
    "    processed_seq_ = list(filter(lambda x : x != \"«»\",processed_seq_))\n",
    "    # shuffle sequence list\n",
    "    samp_idx = np.random.choice(range(len(processed_seq_)), int(len(processed_seq_) * train_ratio), replace=False)\n",
    "    \n",
    "    processed_seq = [processed_seq_[i] for i in samp_idx]\n",
    "    sp_sents = [i.split('^') for i in processed_seq]\n",
    "    \n",
    "    # 8어절로 나눠서 테스트\n",
    "    if isMake_dif_set is True:\n",
    "        n_gram = [[k, v, z, a, c, d, e, f] for sent in sp_sents for k, v, z, a, c, d, e, f in zip(sent, sent[1:], sent[2:],\n",
    "                                                               sent[3:], sent[4:], sent[5:],sent[6:], sent[7:])]\n",
    "    else:\n",
    "        n_gram = sp_sents\n",
    "    \n",
    "    # make_target(space = 1, others = 0)\n",
    "    n_gram_y = y_encoding(n_gram, max_seq_len)\n",
    "    \n",
    "    w2idx, idx2w = load_vocab(vocab_path)\n",
    "    \n",
    "    # input seq - del blank\n",
    "    ngram_encoded_padded = encoding_and_padding(word2idx_dic=w2idx,\n",
    "                                                sequences=[''.join(gram) for gram in n_gram], maxlen=max_seq_len,\n",
    "                                                padding='post', truncating='post')\n",
    "    \n",
    "    if train_ratio < 1:\n",
    "        # make train_set\n",
    "        tr_idx, te_idx = split_train_set(ngram_encoded_padded, train_ratio)\n",
    "\n",
    "        y_train = n_gram_y[tr_idx, ]\n",
    "        x_train = ngram_encoded_padded[tr_idx, ]\n",
    "\n",
    "        y_test = n_gram_y[te_idx, ]\n",
    "        x_test = ngram_encoded_padded[te_idx, ]\n",
    "\n",
    "        # train generator\n",
    "        train_generator = get_transDtype(x_train, y_train, batch_size)\n",
    "        valid_generator = get_transDtype(x_test, y_test, 500)\n",
    "        return (train_generator, valid_generator)\n",
    "    else :\n",
    "        return get_transDtype(ngram_encoded_padded,n_gram_y,batch_size)\n",
    "\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, pad_idx, ctx, n=5000):\n",
    "    # 각 시퀀스의 길이만큼 순회하며 정확도 측정\n",
    "    # 최적화되지 않음\n",
    "    acc = mx.metric.Accuracy(axis=0)\n",
    "    num_of_test = 0\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        # get sentence length\n",
    "        data_np = data.asnumpy()\n",
    "        lengths = np.argmax(np.where(data_np == pad_idx, np.ones_like(data_np),\n",
    "                                     np.zeros_like(data_np)),\n",
    "                            axis=1)\n",
    "        output = net(data)\n",
    "        pred_label = output.squeeze(axis=2) > 0.5\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "            num_of_test += data.shape[0]\n",
    "            acc.update(preds=pred_label[i, :lengths[i]],\n",
    "                       labels=label[i, :lengths[i]])\n",
    "        if num_of_test > n:\n",
    "            break\n",
    "    return acc.get()[1]\n",
    "\n",
    "def train(epochs,train_data,test_data, vali_data, model, loss, trainer,pad_idx,ctx, decay=False,mdl_desc='spacing_model') :\n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "    from mxnet import gluon\n",
    "    import mxnet.autograd as autograd\n",
    "\n",
    "\n",
    "    \n",
    "    outputs = base_path + 'ipynbCode/output/'\n",
    "    mkdirs_(outputs)\n",
    "    \n",
    "    tot_test_acc = []\n",
    "    tot_train_loss = []\n",
    "    \n",
    "    for e in range(epochs) :\n",
    "        tic = time.time()\n",
    "        \n",
    "        if e > 1 and decay :\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.7)\n",
    "            \n",
    "        train_loss = []\n",
    "        \n",
    "        iter_tqdm = tqdm(train_data, 'Batches')\n",
    "        \n",
    "        for  i, (x_data,y_data) in enumerate(iter_tqdm) :\n",
    "            x_data_l = gluon.utils.split_and_load(x_data, ctx, even_split=False)\n",
    "            y_data_l = gluon.utils.split_and_load(y_data, ctx, even_split=False)\n",
    "            \n",
    "            with autograd.record() :\n",
    "                losses = [loss(model(x), y) for x,y in zip(x_data_l, y_data_l)]\n",
    "                \n",
    "            for l in losses :\n",
    "                l.backward()\n",
    "            \n",
    "            trainer.step(x_data.shape[0],ignore_stale_grad=True)\n",
    "            curr_loss = np.mean([mx.nd.mean(l).asscalar() for l in losses])\n",
    "            train_loss.append(curr_loss)\n",
    "            iter_tqdm.set_description(\"loss {}\".format(curr_loss))\n",
    "            mx.nd.waitall()\n",
    "\n",
    "        # caculate test loss\n",
    "        test_acc = evaluate_accuracy(test_data,model,pad_idx,\n",
    "                                     ctx=ctx[0] if isinstance(ctx, list) else mx.gpu(0))\n",
    "        valid_acc = evaluate_accuracy(vali_data,model,pad_idx,\n",
    "                                      ctx=ctx[0] if isinstance(ctx, list) else mx.gpu(0))\n",
    "        logger.info('[Epoch %d] time cost: %f' % (e, time.time() - tic))\n",
    "        logger.info(\"[Epoch %d] Train Loss: %f, Test acc : %f Valid acc : %f\" %\n",
    "                    (e, np.mean(train_loss), test_acc, valid_acc))\n",
    "        tot_test_acc.append(test_acc)\n",
    "        tot_train_loss.append(np.mean(train_loss))\n",
    "        model.save_parameters(outputs + '/' + \"{}_{}.params\".format(mdl_desc, e+1))\n",
    "    return (tot_test_acc, tot_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pfbs8PkkBMZe"
   },
   "outputs": [],
   "source": [
    "# main_run\n",
    "\n",
    "# default_path\n",
    "train_data_path = base_path + 'dataset/train_data/'\n",
    "test_data_path = base_path + 'dataset/test_data/'\n",
    "util_path = base_path + 'ipynbCode/util/'\n",
    "\n",
    "mkdirs_(util_path)\n",
    "\n",
    "w2idx_model = util_path + 'model.mdl'\n",
    "w2idx_embed = util_path + 'emb.np'\n",
    "vocab_path = util_path + 'w2idx.dic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1R1N-gPG5ny"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create embedding_files\n",
    "create_embedding(train_data_path, w2idx_model, w2idx_embed, vocab_path )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2g2lopK5Hnx7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# parameter and dataset, model train\n",
    "w2idx, idx2w = load_vocab(vocab_path)\n",
    "\n",
    "weights = load_embedding(w2idx_embed)\n",
    "\n",
    "vocab_size = weights.shape[0]\n",
    "embed_dim = weights.shape[1]\n",
    "\n",
    "max_seq_len = 300\n",
    "n_hidden = 50\n",
    "gpu_count = 1\n",
    "batch_size = 100\n",
    "ctx = [mx.gpu(i) for i in range(gpu_count)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryjYiKh8IZxk"
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset, modeling, train\n",
    "# train dataset_init\n",
    "train_generator, vali_generator = input_data(train_data_path,train_ratio=0.9,\n",
    "                                             isMake_dif_set=True,max_seq_len = max_seq_len,batch_size=batch_size)\n",
    "\n",
    "# test dataset_init\n",
    "test_generator = input_data(test_data_path)\n",
    "\n",
    "# model_init\n",
    "model, loss, trainer = model_init(n_hidden=n_hidden, vocab_size=vocab_size,\n",
    "                                  embed_dim=embed_dim, max_seq_length=max_seq_len,ctx=ctx, embed_weights=weights)\n",
    "\n",
    "\n",
    "train(epochs=100, train_data=train_generator,test_data=test_generator,\n",
    "      vali_data= vali_generator,model=model,loss=loss,trainer=trainer,\n",
    "      pad_idx=w2idx['__PAD__'],ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Va1xhZYgps0i"
   },
   "outputs": [],
   "source": [
    "# predict_spacing\n",
    "\n",
    "from functools import lru_cache\n",
    "import re\n",
    "\n",
    "class pred_spacing:\n",
    "    def __init__(self, model, w2idx):\n",
    "        self.model = model\n",
    "        self.w2idx = w2idx\n",
    "        self.pattern = re.compile(r'\\s+')\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_spaced_sent(self, raw_sent):\n",
    "        raw_sent_ = \"«\" + raw_sent + \"»\"\n",
    "        raw_sent_ = raw_sent_.replace(' ', '^')\n",
    "        sents_in = [\n",
    "            raw_sent_,\n",
    "        ]\n",
    "        mat_in = encoding_and_padding(word2idx_dic=self.w2idx, sequences=sents_in, maxlen= max_seq_len,\n",
    "                                      padding='post', truncating='post')\n",
    "        mat_in = mx.nd.array(mat_in, ctx=mx.cpu(0))\n",
    "        results = self.model(mat_in)\n",
    "        mat_set = results[0, ]\n",
    "        preds = np.array(\n",
    "            ['1' if i > 0.5 else '0' for i in mat_set[:len(raw_sent_)]])\n",
    "        return self.make_pred_sents(raw_sent_, preds)\n",
    "\n",
    "    def make_pred_sents(self, x_sents, y_pred):\n",
    "        res_sent = []\n",
    "        for i, j in zip(x_sents, y_pred):\n",
    "            if j == '1':\n",
    "                res_sent.append(i)\n",
    "                res_sent.append(' ')\n",
    "            else:\n",
    "                res_sent.append(i)\n",
    "        subs = re.sub(self.pattern, ' ', ''.join(res_sent).replace('^', ' '))\n",
    "        subs = subs.replace('«', '')\n",
    "        subs = subs.replace('»', '')\n",
    "        return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6g2YDX1IKr0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_params_dir = base_path + 'ipynbCode/output/'\n",
    "\n",
    "model_weights = os.listdir(model_params_dir)\n",
    "\n",
    "model_params = model_params_dir + model_weights[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmW-radZps3s"
   },
   "outputs": [],
   "source": [
    "\n",
    "# predict_modeling\n",
    "\n",
    "model = auto_spacing(n_hidden, vocab_size, embed_dim, max_seq_len)\n",
    "\n",
    "# model.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu(0))\n",
    "# model.embedding.weight.set_data(weights)\n",
    "model.load_parameters(model_params, ctx=mx.cpu(0))\n",
    "predictor = pred_spacing(model, w2idx)\n",
    "\n",
    "while 1:\n",
    "    sent = input(\"sent > \")\n",
    "    print(sent)\n",
    "    spaced = predictor.get_spaced_sent(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZB6fHKYzpsyU"
   },
   "outputs": [],
   "source": [
    "logger.info(\"calculate accuracy!\")\n",
    "\n",
    "model = auto_spacing(n_hidden, vocab_size, embed_dim, max_seq_len)\n",
    "\n",
    "# model.initialize(ctx=ctx[0] if isinstance(ctx, list) else mx.gpu(0))\n",
    "model.load_parameters(model_params,\n",
    "                        ctx=ctx[0] if isinstance(ctx, list) else mx.gpu(0))\n",
    "valid_generator = input_data(test_data_path,train_ratio=1, isMake_dif_set=True,max_seq_len = max_seq_len, batch_size=100)\n",
    "\n",
    "\n",
    "valid_acc = evaluate_accuracy(valid_generator, model, w2idx['__PAD__'], ctx=ctx[0] if isinstance(ctx, list) else mx.gpu(0), n=30000)\n",
    "logger.info('valid accuracy : {}'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PgRdmLwO9iH"
   },
   "outputs": [],
   "source": [
    "valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3UW-tEAO9j1"
   },
   "outputs": [],
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PU5j2CwBhaHq"
   },
   "outputs": [],
   "source": [
    "model.collect_params"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SpacingModel_mxnet.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
