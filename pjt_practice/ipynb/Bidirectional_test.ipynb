{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, GRU, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "\n",
    "def make_model(in_,out_) :\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Bidirectional, LSTM, GRU, Dense, Activation, TimeDistributed\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(10, return_sequences=True), input_shape=(in_.shape[1], in_.shape[2])))\n",
    "    model.add(Bidirectional(GRU(10, return_sequences=True)))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>30.478571</td>\n",
       "      <td>30.080000</td>\n",
       "      <td>30.447144</td>\n",
       "      <td>30.104286</td>\n",
       "      <td>88102700.0</td>\n",
       "      <td>20.159719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>30.642857</td>\n",
       "      <td>30.340000</td>\n",
       "      <td>30.490000</td>\n",
       "      <td>30.572857</td>\n",
       "      <td>123432400.0</td>\n",
       "      <td>20.473503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>30.798571</td>\n",
       "      <td>30.464285</td>\n",
       "      <td>30.657143</td>\n",
       "      <td>30.625713</td>\n",
       "      <td>150476200.0</td>\n",
       "      <td>20.508902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>30.747143</td>\n",
       "      <td>30.107143</td>\n",
       "      <td>30.625713</td>\n",
       "      <td>30.138571</td>\n",
       "      <td>138040000.0</td>\n",
       "      <td>20.182680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>30.285715</td>\n",
       "      <td>29.864286</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>30.082857</td>\n",
       "      <td>119282800.0</td>\n",
       "      <td>20.145369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>151.550003</td>\n",
       "      <td>146.589996</td>\n",
       "      <td>148.149994</td>\n",
       "      <td>146.830002</td>\n",
       "      <td>37169200.0</td>\n",
       "      <td>146.830002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>157.229996</td>\n",
       "      <td>146.720001</td>\n",
       "      <td>148.300003</td>\n",
       "      <td>157.169998</td>\n",
       "      <td>58582500.0</td>\n",
       "      <td>157.169998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>156.770004</td>\n",
       "      <td>150.070007</td>\n",
       "      <td>155.839996</td>\n",
       "      <td>156.149994</td>\n",
       "      <td>53117100.0</td>\n",
       "      <td>156.149994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>158.520004</td>\n",
       "      <td>154.550003</td>\n",
       "      <td>157.500000</td>\n",
       "      <td>156.229996</td>\n",
       "      <td>42291400.0</td>\n",
       "      <td>156.229996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>159.360001</td>\n",
       "      <td>156.479996</td>\n",
       "      <td>158.529999</td>\n",
       "      <td>157.740005</td>\n",
       "      <td>35003500.0</td>\n",
       "      <td>157.740005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2265 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close       Volume  \\\n",
       "Date                                                                      \n",
       "2009-12-31   30.478571   30.080000   30.447144   30.104286   88102700.0   \n",
       "2010-01-04   30.642857   30.340000   30.490000   30.572857  123432400.0   \n",
       "2010-01-05   30.798571   30.464285   30.657143   30.625713  150476200.0   \n",
       "2010-01-06   30.747143   30.107143   30.625713   30.138571  138040000.0   \n",
       "2010-01-07   30.285715   29.864286   30.250000   30.082857  119282800.0   \n",
       "...                ...         ...         ...         ...          ...   \n",
       "2018-12-24  151.550003  146.589996  148.149994  146.830002   37169200.0   \n",
       "2018-12-26  157.229996  146.720001  148.300003  157.169998   58582500.0   \n",
       "2018-12-27  156.770004  150.070007  155.839996  156.149994   53117100.0   \n",
       "2018-12-28  158.520004  154.550003  157.500000  156.229996   42291400.0   \n",
       "2018-12-31  159.360001  156.479996  158.529999  157.740005   35003500.0   \n",
       "\n",
       "             Adj Close  \n",
       "Date                    \n",
       "2009-12-31   20.159719  \n",
       "2010-01-04   20.473503  \n",
       "2010-01-05   20.508902  \n",
       "2010-01-06   20.182680  \n",
       "2010-01-07   20.145369  \n",
       "...                ...  \n",
       "2018-12-24  146.830002  \n",
       "2018-12-26  157.169998  \n",
       "2018-12-27  156.149994  \n",
       "2018-12-28  156.229996  \n",
       "2018-12-31  157.740005  \n",
       "\n",
       "[2265 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data/apple_stock.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, index_col='Date')\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_norm = scaler.fit_transform(np.array(data.Close).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s_trans = data_norm[5:].reshape(-1,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = s2s_trans[:,:5,0].reshape(-1,5,1)\n",
    "Y_train_s = s2s_trans[:,5:,0].reshape(-1,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((226, 5, 1), (226, 5, 1))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s.shape, Y_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_48 (Bidirectio (None, 5, 20)             720       \n",
      "_________________________________________________________________\n",
      "bidirectional_49 (Bidirectio (None, 5, 20)             1860      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 5, 1)              21        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 2,601\n",
      "Trainable params: 2,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model(X_train_s,Y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "226/226 [==============================] - 5s 20ms/step - loss: 0.0390\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0033\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0019\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0014\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 9.7118e-04\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.0042e-04\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 5.1768e-04\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 4.3740e-04\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.8185e-04\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 4.5175e-04A: 0s - loss: 4.005\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.3104e-04\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.6826e-04\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.5362e-04\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 3.8819e-04\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.1674e-04\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.9739e-04\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.7774e-04\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.1398e-04\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.6784e-04\n",
      "Epoch 20/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.8084e-04\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.5889e-04\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.0424e-04\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 4.5010e-04\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.1381e-04\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 6.1929e-04\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.5716e-04\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.2436e-04\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.1494e-04\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.6599e-04\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.2544e-04\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.6243e-04\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.3148e-04\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 6.7617e-04\n",
      "Epoch 34/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.5336e-04\n",
      "Epoch 35/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.3431e-04\n",
      "Epoch 36/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.3273e-04\n",
      "Epoch 37/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.8424e-04\n",
      "Epoch 38/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 6.0493e-04\n",
      "Epoch 39/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.3091e-04\n",
      "Epoch 40/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.0588e-04\n",
      "Epoch 41/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.8361e-04\n",
      "Epoch 42/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.9952e-04\n",
      "Epoch 43/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.0817e-04\n",
      "Epoch 44/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.4591e-04\n",
      "Epoch 45/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.5897e-04\n",
      "Epoch 46/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.9540e-04\n",
      "Epoch 47/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.0711e-04\n",
      "Epoch 48/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.9633e-04\n",
      "Epoch 49/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.0600e-04\n",
      "Epoch 50/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.8811e-04\n",
      "Epoch 51/200\n",
      "226/226 [==============================] - 1s 7ms/step - loss: 5.2362e-04\n",
      "Epoch 52/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.4585e-04\n",
      "Epoch 53/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.4527e-04\n",
      "Epoch 54/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.4591e-04\n",
      "Epoch 55/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.2104e-04\n",
      "Epoch 56/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 5.6162e-04\n",
      "Epoch 57/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.3666e-04\n",
      "Epoch 58/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.3556e-04\n",
      "Epoch 59/200\n",
      "226/226 [==============================] - 1s 7ms/step - loss: 4.2164e-04\n",
      "Epoch 60/200\n",
      "226/226 [==============================] - 1s 7ms/step - loss: 5.0547e-04\n",
      "Epoch 61/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 4.6023e-04A: 0s - loss: 4\n",
      "Epoch 62/200\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 5.9867e-04\n",
      "Epoch 63/200\n",
      "226/226 [==============================] - 1s 7ms/step - loss: 5.1121e-04\n",
      "Epoch 64/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.5924e-04\n",
      "Epoch 65/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.6048e-04\n",
      "Epoch 66/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 6.0180e-04\n",
      "Epoch 67/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 3.9357e-04\n",
      "Epoch 68/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.5397e-04\n",
      "Epoch 69/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.0014e-04\n",
      "Epoch 70/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.0941e-04\n",
      "Epoch 71/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.8713e-04\n",
      "Epoch 72/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 5.1926e-04\n",
      "Epoch 73/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 6.4158e-04\n",
      "Epoch 74/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 3.9696e-04\n",
      "Epoch 75/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 5.2573e-04\n",
      "Epoch 76/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 3.9694e-04\n",
      "Epoch 77/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.1784e-04\n",
      "Epoch 78/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.4107e-04\n",
      "Epoch 79/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.6433e-04\n",
      "Epoch 80/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.5007e-04\n",
      "Epoch 81/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.5190e-04\n",
      "Epoch 82/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.3073e-04\n",
      "Epoch 83/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.4887e-04\n",
      "Epoch 84/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.8473e-04\n",
      "Epoch 85/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.6614e-04\n",
      "Epoch 86/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.8136e-04\n",
      "Epoch 87/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.2237e-04\n",
      "Epoch 88/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.8159e-04\n",
      "Epoch 89/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 3.8418e-04\n",
      "Epoch 90/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.3509e-04\n",
      "Epoch 91/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.1819e-04\n",
      "Epoch 92/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.7600e-04\n",
      "Epoch 93/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.6550e-04\n",
      "Epoch 94/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.7913e-04\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 2s 7ms/step - loss: 4.0998e-04\n",
      "Epoch 96/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.2658e-04\n",
      "Epoch 97/200\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 4.6536e-04\n",
      "Epoch 98/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.2737e-04\n",
      "Epoch 99/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.2300e-04\n",
      "Epoch 100/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 3.8816e-04\n",
      "Epoch 101/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 3.8571e-04\n",
      "Epoch 102/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 3.8900e-04\n",
      "Epoch 103/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.7090e-04\n",
      "Epoch 104/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 3.9231e-04\n",
      "Epoch 105/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 5.3237e-04\n",
      "Epoch 106/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.9972e-04\n",
      "Epoch 107/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.2918e-04\n",
      "Epoch 108/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.3482e-04\n",
      "Epoch 109/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.9604e-04\n",
      "Epoch 110/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.1203e-04\n",
      "Epoch 111/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.3049e-04\n",
      "Epoch 112/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 5.5339e-04\n",
      "Epoch 113/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.5289e-04\n",
      "Epoch 114/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 3.9962e-04\n",
      "Epoch 115/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.8585e-04\n",
      "Epoch 116/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 3.7185e-04\n",
      "Epoch 117/200\n",
      "226/226 [==============================] - 2s 8ms/step - loss: 4.4157e-04\n",
      "Epoch 118/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.7174e-04\n",
      "Epoch 119/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.5067e-04\n",
      "Epoch 120/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.1755e-04\n",
      "Epoch 121/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 3.9025e-04\n",
      "Epoch 122/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 3.9123e-04\n",
      "Epoch 123/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.2921e-04\n",
      "Epoch 124/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.3997e-04\n",
      "Epoch 125/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.4835e-04\n",
      "Epoch 126/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 3.9689e-04\n",
      "Epoch 127/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.3196e-04\n",
      "Epoch 128/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.2968e-04\n",
      "Epoch 129/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 3.9794e-04\n",
      "Epoch 130/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.3426e-04\n",
      "Epoch 131/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.4391e-04\n",
      "Epoch 132/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.3524e-04\n",
      "Epoch 133/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.7710e-04\n",
      "Epoch 134/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.1140e-04\n",
      "Epoch 135/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.6023e-04\n",
      "Epoch 136/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 3.9855e-04\n",
      "Epoch 137/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 3.9545e-04\n",
      "Epoch 138/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.9808e-04\n",
      "Epoch 139/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 3.6869e-04\n",
      "Epoch 140/200\n",
      "226/226 [==============================] - 2s 9ms/step - loss: 4.9621e-04\n",
      "Epoch 141/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.0893e-04\n",
      "Epoch 142/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.5744e-04\n",
      "Epoch 143/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 3.9050e-04\n",
      "Epoch 144/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.1247e-04\n",
      "Epoch 145/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 3.6507e-04\n",
      "Epoch 146/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.0439e-04\n",
      "Epoch 147/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.0161e-04\n",
      "Epoch 148/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 3.9719e-04\n",
      "Epoch 149/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 4.4120e-04\n",
      "Epoch 150/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 3.8633e-04\n",
      "Epoch 151/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.0403e-04\n",
      "Epoch 152/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.5718e-04\n",
      "Epoch 153/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 3.8301e-04\n",
      "Epoch 154/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.7396e-04\n",
      "Epoch 155/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 4.3127e-04\n",
      "Epoch 156/200\n",
      "226/226 [==============================] - 2s 10ms/step - loss: 4.1083e-04\n",
      "Epoch 157/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 3.9522e-04\n",
      "Epoch 158/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 4.9463e-04\n",
      "Epoch 159/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 4.1049e-04\n",
      "Epoch 160/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 3.9032e-04\n",
      "Epoch 161/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 4.2224e-04\n",
      "Epoch 162/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 3.9858e-04\n",
      "Epoch 163/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 3.6865e-04\n",
      "Epoch 164/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 5.4709e-04\n",
      "Epoch 165/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 3.9556e-04\n",
      "Epoch 166/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 4.6279e-04\n",
      "Epoch 167/200\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 4.2158e-04\n",
      "Epoch 168/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 3.8384e-04\n",
      "Epoch 169/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 3.8168e-04\n",
      "Epoch 170/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 3.9700e-04\n",
      "Epoch 171/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 4.2168e-04\n",
      "Epoch 172/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 3.9376e-04\n",
      "Epoch 173/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 3.7373e-04\n",
      "Epoch 174/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 5.0332e-04\n",
      "Epoch 175/200\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 3.8194e-04\n",
      "Epoch 176/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 4.0170e-04\n",
      "Epoch 177/200\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.0934e-04\n",
      "Epoch 178/200\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.1131e-04\n",
      "Epoch 179/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 3.9545e-04\n",
      "Epoch 180/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 4.1149e-04\n",
      "Epoch 181/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 3.9775e-04\n",
      "Epoch 182/200\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.4445e-04\n",
      "Epoch 183/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 3.4318e-04\n",
      "Epoch 184/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 3.9877e-04\n",
      "Epoch 185/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 4.2818e-04\n",
      "Epoch 186/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 4.6905e-04\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 3s 12ms/step - loss: 4.8517e-04\n",
      "Epoch 188/200\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 4.1078e-04\n",
      "Epoch 189/200\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.7722e-04\n",
      "Epoch 190/200\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 4.4125e-04\n",
      "Epoch 191/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.6115e-04\n",
      "Epoch 192/200\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 4.0194e-04\n",
      "Epoch 193/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.9560e-04\n",
      "Epoch 194/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.5590e-04\n",
      "Epoch 195/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.6504e-04\n",
      "Epoch 196/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.0880e-04\n",
      "Epoch 197/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.6768e-04\n",
      "Epoch 198/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.6434e-04\n",
      "Epoch 199/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.7545e-04\n",
      "Epoch 200/200\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.9181e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_s, Y_train_s, epochs=200,batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30.78023 ]\n",
      " [30.275667]\n",
      " [30.098192]\n",
      " [29.790064]\n",
      " [29.923487]]\n",
      "\n",
      "[[27.81857109]\n",
      " [27.97999954]\n",
      " [28.46142769]\n",
      " [27.43571472]\n",
      " [27.92285728]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = scaler.inverse_transform(model.predict( s2s_trans[1, 0:5, 0].reshape(-1,5,1)).reshape(-1,1))\n",
    "print(y_predict)\n",
    "print()\n",
    "print(scaler.inverse_transform(np.array(s2s_trans[1, 5:10, 0]).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48.57631 ]\n",
      " [48.79808 ]\n",
      " [49.00249 ]\n",
      " [49.012104]\n",
      " [49.04175 ]]\n",
      "\n",
      "[[50.43000031]\n",
      " [50.06000137]\n",
      " [50.02142715]\n",
      " [49.5357132 ]\n",
      " [50.01856995]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = scaler.inverse_transform(model.predict( s2s_trans[32, 0:5, 0].reshape(-1,5,1)).reshape(-1,1))\n",
    "print(y_predict)\n",
    "print()\n",
    "print(scaler.inverse_transform(np.array(s2s_trans[32, 5:10, 0]).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm2 = data_norm.reshape(-1,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n = data_norm2[:data_norm2.shape[0],0:4,0].reshape(-1,4,1)\n",
    "Y_train_n = data_norm2[:data_norm2.shape[0],4,0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_() :\n",
    "    model = Sequential()\n",
    "    model.add(GRU(10, activation='linear', input_shape=(X_train.shape[1],X_train.shape[2]))) \n",
    "    model.add(Dense(1))       \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "\n",
    "def make_model_2(in_,out_) :\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Bidirectional, LSTM, GRU, Dense, Activation, TimeDistributed\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(10, return_sequences=True), input_shape=(in_.shape[1], in_.shape[2])))\n",
    "    model.add(Bidirectional(GRU(10)))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile( loss='mse', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_38 (Bidirectio (None, 4, 20)             720       \n",
      "_________________________________________________________________\n",
      "bidirectional_39 (Bidirectio (None, 20)                1860      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 21        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,601\n",
      "Trainable params: 2,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 362 samples, validate on 91 samples\n",
      "Epoch 1/200\n",
      "362/362 [==============================] - 3s 9ms/step - loss: 0.0339 - val_loss: 0.1107\n",
      "Epoch 2/200\n",
      "362/362 [==============================] - 0s 293us/step - loss: 0.0065 - val_loss: 0.0249\n",
      "Epoch 3/200\n",
      "362/362 [==============================] - 0s 279us/step - loss: 0.0057 - val_loss: 0.0309\n",
      "Epoch 4/200\n",
      "362/362 [==============================] - 0s 284us/step - loss: 0.0032 - val_loss: 0.0361\n",
      "Epoch 5/200\n",
      "362/362 [==============================] - 0s 266us/step - loss: 0.0024 - val_loss: 0.0214\n",
      "Epoch 6/200\n",
      "362/362 [==============================] - 0s 288us/step - loss: 0.0016 - val_loss: 0.0110\n",
      "Epoch 7/200\n",
      "362/362 [==============================] - 0s 266us/step - loss: 9.3131e-04 - val_loss: 0.0082\n",
      "Epoch 8/200\n",
      "362/362 [==============================] - 0s 271us/step - loss: 5.0509e-04 - val_loss: 0.0039\n",
      "Epoch 9/200\n",
      "362/362 [==============================] - 0s 271us/step - loss: 2.5470e-04 - val_loss: 0.0019\n",
      "Epoch 10/200\n",
      "362/362 [==============================] - 0s 267us/step - loss: 1.3488e-04 - val_loss: 0.0010\n",
      "Epoch 11/200\n",
      "362/362 [==============================] - 0s 261us/step - loss: 9.0907e-05 - val_loss: 6.1998e-04\n",
      "Epoch 12/200\n",
      "362/362 [==============================] - 0s 267us/step - loss: 8.0728e-05 - val_loss: 5.1234e-04\n",
      "Epoch 13/200\n",
      "362/362 [==============================] - 0s 260us/step - loss: 7.9198e-05 - val_loss: 4.6613e-04\n",
      "Epoch 14/200\n",
      "362/362 [==============================] - 0s 263us/step - loss: 7.9914e-05 - val_loss: 4.6409e-04\n",
      "Epoch 15/200\n",
      "362/362 [==============================] - 0s 229us/step - loss: 7.9513e-05 - val_loss: 4.6999e-04\n",
      "Epoch 16/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.1220e-05 - val_loss: 4.8001e-04\n",
      "Epoch 17/200\n",
      "362/362 [==============================] - 0s 229us/step - loss: 7.9898e-05 - val_loss: 4.9106e-04\n",
      "Epoch 18/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.8889e-05 - val_loss: 4.6689e-04\n",
      "Epoch 19/200\n",
      "362/362 [==============================] - 0s 221us/step - loss: 7.9402e-05 - val_loss: 4.8188e-04\n",
      "Epoch 20/200\n",
      "362/362 [==============================] - 0s 218us/step - loss: 7.9218e-05 - val_loss: 4.7104e-04\n",
      "Epoch 21/200\n",
      "362/362 [==============================] - 0s 221us/step - loss: 7.9220e-05 - val_loss: 4.8341e-04\n",
      "Epoch 22/200\n",
      "362/362 [==============================] - 0s 229us/step - loss: 7.9449e-05 - val_loss: 5.0718e-04\n",
      "Epoch 23/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.9550e-05 - val_loss: 4.6354e-04\n",
      "Epoch 24/200\n",
      "362/362 [==============================] - 0s 232us/step - loss: 8.0584e-05 - val_loss: 5.0184e-04\n",
      "Epoch 25/200\n",
      "362/362 [==============================] - 0s 236us/step - loss: 7.8853e-05 - val_loss: 4.7310e-04\n",
      "Epoch 26/200\n",
      "362/362 [==============================] - 0s 227us/step - loss: 8.0209e-05 - val_loss: 4.7499e-04\n",
      "Epoch 27/200\n",
      "362/362 [==============================] - 0s 229us/step - loss: 8.0497e-05 - val_loss: 5.1105e-04\n",
      "Epoch 28/200\n",
      "362/362 [==============================] - 0s 220us/step - loss: 7.9716e-05 - val_loss: 4.6603e-04\n",
      "Epoch 29/200\n",
      "362/362 [==============================] - 0s 223us/step - loss: 7.9137e-05 - val_loss: 5.0143e-04\n",
      "Epoch 30/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.0374e-05 - val_loss: 4.6330e-04\n",
      "Epoch 31/200\n",
      "362/362 [==============================] - 0s 218us/step - loss: 7.9283e-05 - val_loss: 5.0144e-04\n",
      "Epoch 32/200\n",
      "362/362 [==============================] - 0s 212us/step - loss: 8.0716e-05 - val_loss: 5.3202e-04\n",
      "Epoch 33/200\n",
      "362/362 [==============================] - 0s 207us/step - loss: 9.2577e-05 - val_loss: 4.4868e-04\n",
      "Epoch 34/200\n",
      "362/362 [==============================] - 0s 207us/step - loss: 8.0842e-05 - val_loss: 4.9106e-04\n",
      "Epoch 35/200\n",
      "362/362 [==============================] - 0s 212us/step - loss: 7.9691e-05 - val_loss: 4.9997e-04\n",
      "Epoch 36/200\n",
      "362/362 [==============================] - 0s 207us/step - loss: 8.2289e-05 - val_loss: 4.4098e-04\n",
      "Epoch 37/200\n",
      "362/362 [==============================] - 0s 198us/step - loss: 8.1528e-05 - val_loss: 4.7866e-04\n",
      "Epoch 38/200\n",
      "362/362 [==============================] - 0s 193us/step - loss: 7.9924e-05 - val_loss: 4.7004e-04\n",
      "Epoch 39/200\n",
      "362/362 [==============================] - 0s 201us/step - loss: 8.0480e-05 - val_loss: 5.3114e-04\n",
      "Epoch 40/200\n",
      "362/362 [==============================] - 0s 220us/step - loss: 8.0196e-05 - val_loss: 4.7944e-04\n",
      "Epoch 41/200\n",
      "362/362 [==============================] - 0s 215us/step - loss: 7.9724e-05 - val_loss: 4.6558e-04\n",
      "Epoch 42/200\n",
      "362/362 [==============================] - 0s 212us/step - loss: 7.9330e-05 - val_loss: 4.7269e-04\n",
      "Epoch 43/200\n",
      "362/362 [==============================] - 0s 220us/step - loss: 7.9583e-05 - val_loss: 4.2993e-04\n",
      "Epoch 44/200\n",
      "362/362 [==============================] - 0s 226us/step - loss: 8.2138e-05 - val_loss: 4.8709e-04\n",
      "Epoch 45/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.1105e-05 - val_loss: 5.5691e-04\n",
      "Epoch 46/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.5614e-05 - val_loss: 4.7588e-04\n",
      "Epoch 47/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.2823e-05 - val_loss: 4.5768e-04\n",
      "Epoch 48/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.0025e-05 - val_loss: 4.6946e-04\n",
      "Epoch 49/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.9277e-05 - val_loss: 4.8017e-04\n",
      "Epoch 50/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.8641e-05 - val_loss: 4.7293e-04\n",
      "Epoch 51/200\n",
      "362/362 [==============================] - 0s 231us/step - loss: 7.8378e-05 - val_loss: 4.8000e-04\n",
      "Epoch 52/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.8551e-05 - val_loss: 4.6119e-04\n",
      "Epoch 53/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.9889e-05 - val_loss: 4.9256e-04\n",
      "Epoch 54/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.8435e-05 - val_loss: 4.7899e-04\n",
      "Epoch 55/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.8186e-05 - val_loss: 4.6183e-04\n",
      "Epoch 56/200\n",
      "362/362 [==============================] - 0s 244us/step - loss: 7.9430e-05 - val_loss: 5.6261e-04\n",
      "Epoch 57/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.4307e-05 - val_loss: 5.0022e-04\n",
      "Epoch 58/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.9369e-05 - val_loss: 4.4234e-04\n",
      "Epoch 59/200\n",
      "362/362 [==============================] - 0s 234us/step - loss: 7.9708e-05 - val_loss: 4.7202e-04\n",
      "Epoch 60/200\n",
      "362/362 [==============================] - 0s 231us/step - loss: 7.8196e-05 - val_loss: 4.7673e-04\n",
      "Epoch 61/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.8512e-05 - val_loss: 4.2331e-04\n",
      "Epoch 62/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.5185e-05 - val_loss: 4.4249e-04\n",
      "Epoch 63/200\n",
      "362/362 [==============================] - 0s 234us/step - loss: 8.1214e-05 - val_loss: 6.3246e-04\n",
      "Epoch 64/200\n",
      "362/362 [==============================] - 0s 233us/step - loss: 9.1475e-05 - val_loss: 4.8211e-04\n",
      "Epoch 65/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 8.3831e-05 - val_loss: 3.9162e-04\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/362 [==============================] - 0s 242us/step - loss: 8.7233e-05 - val_loss: 5.6658e-04\n",
      "Epoch 67/200\n",
      "362/362 [==============================] - 0s 232us/step - loss: 7.7140e-05 - val_loss: 4.1453e-04\n",
      "Epoch 68/200\n",
      "362/362 [==============================] - 0s 233us/step - loss: 7.9707e-05 - val_loss: 4.4117e-04\n",
      "Epoch 69/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.9204e-05 - val_loss: 4.6334e-04\n",
      "Epoch 70/200\n",
      "362/362 [==============================] - 0s 232us/step - loss: 7.8511e-05 - val_loss: 4.1996e-04\n",
      "Epoch 71/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.0099e-05 - val_loss: 4.6712e-04\n",
      "Epoch 72/200\n",
      "362/362 [==============================] - 0s 234us/step - loss: 7.9988e-05 - val_loss: 5.3333e-04\n",
      "Epoch 73/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.8794e-05 - val_loss: 5.1292e-04\n",
      "Epoch 74/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.8386e-05 - val_loss: 4.9278e-04\n",
      "Epoch 75/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.9933e-05 - val_loss: 4.5655e-04\n",
      "Epoch 76/200\n",
      "362/362 [==============================] - 0s 234us/step - loss: 7.7201e-05 - val_loss: 4.9827e-04\n",
      "Epoch 77/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.7562e-05 - val_loss: 4.3764e-04\n",
      "Epoch 78/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.0237e-05 - val_loss: 4.3423e-04\n",
      "Epoch 79/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 8.3199e-05 - val_loss: 5.8460e-04\n",
      "Epoch 80/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.9305e-05 - val_loss: 5.0626e-04\n",
      "Epoch 81/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.7060e-05 - val_loss: 4.2399e-04\n",
      "Epoch 82/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.8592e-05 - val_loss: 4.9559e-04\n",
      "Epoch 83/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.8403e-05 - val_loss: 4.9382e-04\n",
      "Epoch 84/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.0137e-05 - val_loss: 4.4441e-04\n",
      "Epoch 85/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.8702e-05 - val_loss: 4.2214e-04\n",
      "Epoch 86/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.8835e-05 - val_loss: 4.6316e-04\n",
      "Epoch 87/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.8723e-05 - val_loss: 5.1762e-04\n",
      "Epoch 88/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.1725e-05 - val_loss: 4.5070e-04\n",
      "Epoch 89/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.6531e-05 - val_loss: 5.3460e-04\n",
      "Epoch 90/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.7193e-05 - val_loss: 5.1737e-04\n",
      "Epoch 91/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.8790e-05 - val_loss: 5.3503e-04\n",
      "Epoch 92/200\n",
      "362/362 [==============================] - 0s 243us/step - loss: 8.1735e-05 - val_loss: 4.4838e-04\n",
      "Epoch 93/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.5647e-05 - val_loss: 5.1247e-04\n",
      "Epoch 94/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.6277e-05 - val_loss: 4.3734e-04\n",
      "Epoch 95/200\n",
      "362/362 [==============================] - 0s 241us/step - loss: 7.7234e-05 - val_loss: 5.2076e-04\n",
      "Epoch 96/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.6949e-05 - val_loss: 4.5011e-04\n",
      "Epoch 97/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.1765e-05 - val_loss: 4.2577e-04\n",
      "Epoch 98/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.6909e-05 - val_loss: 4.4248e-04\n",
      "Epoch 99/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.7463e-05 - val_loss: 4.2777e-04\n",
      "Epoch 100/200\n",
      "362/362 [==============================] - 0s 241us/step - loss: 7.9134e-05 - val_loss: 5.0493e-04\n",
      "Epoch 101/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.7416e-05 - val_loss: 4.7757e-04\n",
      "Epoch 102/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.9058e-05 - val_loss: 4.7066e-04\n",
      "Epoch 103/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 8.2116e-05 - val_loss: 4.9592e-04\n",
      "Epoch 104/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.1885e-05 - val_loss: 4.8937e-04\n",
      "Epoch 105/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.6568e-05 - val_loss: 5.9497e-04\n",
      "Epoch 106/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.4376e-05 - val_loss: 4.6178e-04\n",
      "Epoch 107/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.8387e-05 - val_loss: 5.7030e-04\n",
      "Epoch 108/200\n",
      "362/362 [==============================] - 0s 238us/step - loss: 8.2850e-05 - val_loss: 6.1715e-04\n",
      "Epoch 109/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.9289e-05 - val_loss: 4.5667e-04\n",
      "Epoch 110/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.6991e-05 - val_loss: 3.9998e-04\n",
      "Epoch 111/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.9126e-05 - val_loss: 4.9847e-04\n",
      "Epoch 112/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.6078e-05 - val_loss: 4.9932e-04\n",
      "Epoch 113/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.8339e-05 - val_loss: 5.0701e-04\n",
      "Epoch 114/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.6611e-05 - val_loss: 5.2704e-04\n",
      "Epoch 115/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.6600e-05 - val_loss: 4.8490e-04\n",
      "Epoch 116/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.7661e-05 - val_loss: 5.1106e-04\n",
      "Epoch 117/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.7331e-05 - val_loss: 4.5837e-04\n",
      "Epoch 118/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.8681e-05 - val_loss: 4.5300e-04\n",
      "Epoch 119/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 8.1411e-05 - val_loss: 3.9858e-04\n",
      "Epoch 120/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.7767e-05 - val_loss: 4.3611e-04\n",
      "Epoch 121/200\n",
      "362/362 [==============================] - 0s 232us/step - loss: 7.5704e-05 - val_loss: 4.7197e-04\n",
      "Epoch 122/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.4992e-05 - val_loss: 4.4380e-04\n",
      "Epoch 123/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.8344e-05 - val_loss: 5.8450e-04\n",
      "Epoch 124/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.7907e-05 - val_loss: 5.7871e-04\n",
      "Epoch 125/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.3929e-05 - val_loss: 5.8585e-04\n",
      "Epoch 126/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.7002e-05 - val_loss: 4.9933e-04\n",
      "Epoch 127/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.5245e-05 - val_loss: 4.3128e-04\n",
      "Epoch 128/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.5157e-05 - val_loss: 4.1515e-04\n",
      "Epoch 129/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.2095e-05 - val_loss: 4.3054e-04\n",
      "Epoch 130/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 8.0321e-05 - val_loss: 4.2863e-04\n",
      "Epoch 131/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.5946e-05 - val_loss: 5.5430e-04\n",
      "Epoch 132/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.8175e-05 - val_loss: 5.4667e-04\n",
      "Epoch 133/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.5952e-05 - val_loss: 5.0075e-04\n",
      "Epoch 134/200\n",
      "362/362 [==============================] - 0s 234us/step - loss: 7.6465e-05 - val_loss: 4.1303e-04\n",
      "Epoch 135/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.8324e-05 - val_loss: 3.7270e-04\n",
      "Epoch 136/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 8.1115e-05 - val_loss: 4.6404e-04\n",
      "Epoch 137/200\n",
      "362/362 [==============================] - 0s 243us/step - loss: 7.3482e-05 - val_loss: 4.0347e-04\n",
      "Epoch 138/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.7477e-05 - val_loss: 4.4999e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 8.2471e-05 - val_loss: 3.7172e-04\n",
      "Epoch 140/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.2720e-05 - val_loss: 4.2714e-04\n",
      "Epoch 141/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.3165e-05 - val_loss: 5.2856e-04\n",
      "Epoch 142/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.5323e-05 - val_loss: 5.5187e-04\n",
      "Epoch 143/200\n",
      "362/362 [==============================] - 0s 231us/step - loss: 8.0428e-05 - val_loss: 5.5854e-04\n",
      "Epoch 144/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.9261e-05 - val_loss: 5.6973e-04\n",
      "Epoch 145/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.4984e-05 - val_loss: 4.9066e-04\n",
      "Epoch 146/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.3208e-05 - val_loss: 4.7192e-04\n",
      "Epoch 147/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.5966e-05 - val_loss: 3.8652e-04\n",
      "Epoch 148/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.5315e-05 - val_loss: 5.0388e-04\n",
      "Epoch 149/200\n",
      "362/362 [==============================] - 0s 243us/step - loss: 7.2404e-05 - val_loss: 4.2044e-04\n",
      "Epoch 150/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.1603e-05 - val_loss: 5.8819e-04\n",
      "Epoch 151/200\n",
      "362/362 [==============================] - 0s 234us/step - loss: 7.5222e-05 - val_loss: 5.6577e-04\n",
      "Epoch 152/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.6374e-05 - val_loss: 4.5526e-04\n",
      "Epoch 153/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.4539e-05 - val_loss: 3.9971e-04\n",
      "Epoch 154/200\n",
      "362/362 [==============================] - 0s 241us/step - loss: 7.6420e-05 - val_loss: 4.1167e-04\n",
      "Epoch 155/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 7.8519e-05 - val_loss: 4.4683e-04\n",
      "Epoch 156/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.3940e-05 - val_loss: 5.6330e-04\n",
      "Epoch 157/200\n",
      "362/362 [==============================] - 0s 240us/step - loss: 7.3094e-05 - val_loss: 6.7357e-04\n",
      "Epoch 158/200\n",
      "362/362 [==============================] - 0s 237us/step - loss: 8.5771e-05 - val_loss: 5.4471e-04\n",
      "Epoch 159/200\n",
      "362/362 [==============================] - 0s 246us/step - loss: 7.2801e-05 - val_loss: 4.6311e-04\n",
      "Epoch 160/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.4532e-05 - val_loss: 3.8241e-04\n",
      "Epoch 161/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 7.9176e-05 - val_loss: 5.0772e-04\n",
      "Epoch 162/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 8.6856e-05 - val_loss: 4.8992e-04\n",
      "Epoch 163/200\n",
      "362/362 [==============================] - 0s 239us/step - loss: 7.2529e-05 - val_loss: 5.4303e-04\n",
      "Epoch 164/200\n",
      "362/362 [==============================] - 0s 234us/step - loss: 7.3593e-05 - val_loss: 6.2148e-04\n",
      "Epoch 165/200\n",
      "362/362 [==============================] - 0s 254us/step - loss: 7.6078e-05 - val_loss: 5.9902e-04\n",
      "Epoch 166/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 8.4829e-05 - val_loss: 6.5507e-04\n",
      "Epoch 167/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 8.0320e-05 - val_loss: 6.8362e-04\n",
      "Epoch 168/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.7567e-05 - val_loss: 5.4767e-04\n",
      "Epoch 169/200\n",
      "362/362 [==============================] - 0s 251us/step - loss: 7.4383e-05 - val_loss: 4.5641e-04\n",
      "Epoch 170/200\n",
      "362/362 [==============================] - 0s 254us/step - loss: 7.1414e-05 - val_loss: 4.5430e-04\n",
      "Epoch 171/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 7.4040e-05 - val_loss: 3.7216e-04\n",
      "Epoch 172/200\n",
      "362/362 [==============================] - 0s 242us/step - loss: 8.0884e-05 - val_loss: 4.2589e-04\n",
      "Epoch 173/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 7.2849e-05 - val_loss: 4.2033e-04\n",
      "Epoch 174/200\n",
      "362/362 [==============================] - 0s 250us/step - loss: 7.2006e-05 - val_loss: 4.4705e-04\n",
      "Epoch 175/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 7.2937e-05 - val_loss: 4.1509e-04\n",
      "Epoch 176/200\n",
      "362/362 [==============================] - 0s 251us/step - loss: 7.4792e-05 - val_loss: 4.2714e-04\n",
      "Epoch 177/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.3380e-05 - val_loss: 5.2659e-04\n",
      "Epoch 178/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 7.8610e-05 - val_loss: 4.9747e-04\n",
      "Epoch 179/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.0673e-05 - val_loss: 5.0607e-04\n",
      "Epoch 180/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.2876e-05 - val_loss: 5.3466e-04\n",
      "Epoch 181/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.4982e-05 - val_loss: 5.0848e-04\n",
      "Epoch 182/200\n",
      "362/362 [==============================] - 0s 251us/step - loss: 7.3420e-05 - val_loss: 4.3972e-04\n",
      "Epoch 183/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 7.2618e-05 - val_loss: 4.9017e-04\n",
      "Epoch 184/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 7.7819e-05 - val_loss: 5.5681e-04\n",
      "Epoch 185/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.6099e-05 - val_loss: 4.2193e-04\n",
      "Epoch 186/200\n",
      "362/362 [==============================] - 0s 251us/step - loss: 7.6111e-05 - val_loss: 3.6289e-04\n",
      "Epoch 187/200\n",
      "362/362 [==============================] - 0s 257us/step - loss: 7.4832e-05 - val_loss: 3.8739e-04\n",
      "Epoch 188/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.7959e-05 - val_loss: 7.3707e-04\n",
      "Epoch 189/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 8.7204e-05 - val_loss: 6.2438e-04\n",
      "Epoch 190/200\n",
      "362/362 [==============================] - 0s 253us/step - loss: 7.2144e-05 - val_loss: 6.2286e-04\n",
      "Epoch 191/200\n",
      "362/362 [==============================] - 0s 256us/step - loss: 8.4531e-05 - val_loss: 5.3313e-04\n",
      "Epoch 192/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.9193e-05 - val_loss: 3.6616e-04\n",
      "Epoch 193/200\n",
      "362/362 [==============================] - 0s 245us/step - loss: 7.7673e-05 - val_loss: 4.7309e-04\n",
      "Epoch 194/200\n",
      "362/362 [==============================] - 0s 243us/step - loss: 7.2191e-05 - val_loss: 4.1986e-04\n",
      "Epoch 195/200\n",
      "362/362 [==============================] - 0s 258us/step - loss: 7.0343e-05 - val_loss: 4.5309e-04\n",
      "Epoch 196/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.0382e-05 - val_loss: 4.4838e-04\n",
      "Epoch 197/200\n",
      "362/362 [==============================] - 0s 251us/step - loss: 7.4536e-05 - val_loss: 4.4968e-04\n",
      "Epoch 198/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 7.0148e-05 - val_loss: 4.8263e-04\n",
      "Epoch 199/200\n",
      "362/362 [==============================] - 0s 259us/step - loss: 6.8827e-05 - val_loss: 6.3699e-04\n",
      "Epoch 200/200\n",
      "362/362 [==============================] - 0s 248us/step - loss: 8.3190e-05 - val_loss: 6.2254e-04\n"
     ]
    }
   ],
   "source": [
    "# model fitting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model_n = make_model_2(X_train_n,Y_train_n)\n",
    "\n",
    "history = model_n.fit(X_train_n, Y_train_n, epochs = 200, validation_split = 0.2, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.title('loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27.760244]] [[27.92285728]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = scaler.inverse_transform(model_n.predict( data_norm2[4, 0:4, 0].reshape(-1,4,1)))\n",
    "print(y_predict,   scaler.inverse_transform(np.array(data_norm2[4 , 4, 0 ]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29.844692]] [[29.91857147]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = scaler.inverse_transform(model_n.predict( data_norm2[1, 0:4, 0].reshape(-1,4,1)))\n",
    "print(y_predict,   scaler.inverse_transform(np.array(data_norm2[1 , 4, 0 ]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36.223736]] [[37.70714188]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = scaler.inverse_transform(model_n.predict( data_norm2[20, 0:4, 0].reshape(-1,4,1)))\n",
    "print(y_predict,   scaler.inverse_transform(np.array(data_norm2[20, 4, 0 ]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38.52457]] [[39.33856964]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = scaler.inverse_transform(model_n.predict( data_norm2[35, 0:4, 0].reshape(-1,4,1)))\n",
    "print(y_predict,   scaler.inverse_transform(np.array(data_norm2[35, 4, 0 ]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
